# GomokuAI
## 1. 개요

    2023년 2월 강화학습 스터디 프로젝트입니다.
    오목 게임을 통해 강화학습을 구현합니다.
    
    오목 게임은 15x15의 보드에서 5개의 돌을 먼저 놓은 사람이 승리하는 게임입니다.
    
    여러 강화학습 알고리즘들을 구현하고, 각 알고리즘들의 성능을 비교합니다.
    사람과도 플레이 할 수 있도록 합니다.

## 2. 오목 환경 구성
    1) 오목 base 클래스
    2) 사람 vs 사람
    3) 서로 AI를 맞대어 학습
    4) Yixin으로 AI 학습
        (1) 그 후에 "3)" 수행

## 3. 오목 base 클래스
    1) step 메서드
    2) 승패 판정 메서드
    3) Judge 메서드
    4) pygame render 메서드
    5) 기타 기능

## 4. Judge 메서드
    reward 반환하는 것과 직결되어 있기 때문에 잘 설계해야 합니다.
    reward 반환 방식은 "나"가 착수한 뒤 "상대"가 착수하고 나서의 상태를 확인해보는 것입니다.

    3가지 방식을 고민해 볼 수 있습니다.
    1) 오목 게임이 끝났을 때
    2) 직관적으로 승패에 근접한 상황일 때
        (1) "나" 4목
        (2) "상대" 열린 4목
        (3) "상대" 닫힌 4목
        (4) "나" 열린 3목
        (5) "상대" 열린 3목
    3) Yixin의 judge 값 참조

## 5. DQN 모델
    앞으로 구체적으로 계획할 단계입니다.

## 6. Inference
    1) AI vs 사람
    2) AI vs Yixin


## 7. 일정
||~2/10|~2/15|~2/17|~2/21|~2/24|
| --- | --- | --- | --- | --- | --- | 
| 공통 | 오목 환경 구성 | | 모델 구조 설계 및 완성| 추가 모델 구현 | 모델 성능 비교 및 정리 |
| 윤대헌 | 사람 vs 사람 환경 구성 |Reward 우선순위 설계|
| 백보성 | 기획서 작성 |학습 구조 만들기|
---
---
## 2. 구현
### Env는 다음과 같이 구성됩니다.
##
    1) 보드
        보드는 15x15의 2차원 np.array로 구성됩니다.
        보드의 각 원소는 0, 1, -1로 구성되며, 0은 비어있음을 의미하고, 1은 흑돌, -1은 백돌을 의미합니다.
        step()을 받게 되면 자동으로 다음 돌로 변경하여 보드를 업데이트 합니다.
##
    2) Reward
        Reward는 이겼을 때의 경우, 승리에 근접한 경우, 패배한 경우, 패배에 근접한 경우, 그 외의 경우로 나누어 구현합니다.
        이겼을 때의 경우, 승리에 근접한 경우, 패배한 경우, 패배에 근접한 경우는 각각 1, 0.5, -1, -0.5의 reward를 받습니다.
        
        해당 경우들의 여부는 다음과 같이 판단합니다.
        1) 이겼을 때의 경우
            현재 보드에서 돌이 5개 이상 연속으로 놓여있는 경우
        
        2) 승리에 근접한 경우
            1. 현재 보드에서 돌이 반 닫힌 상태로 4개 이상 연속으로 놓여있는 경우,
            2. 현재 보드에서 돌이 모두 열린 상태로 3개 연속으로 놓여있는 경우,
            3. 닫힘 여부와 상관 없이 3-3이 완성되는 경우

        3) 패배한 경우
            현재 보드에서 상대방의 돌이 5개 이상 연속으로 놓여있는 경우

        4) 패배에 근접한 경우
            2) 승리에 근접한 경우가 상대방에게 적용된 경우
##
    3) Done
        Done은 다음과 같이 판단합니다.
        1) 현재 보드에서 돌이 5개 이상 연속으로 놓여있는 경우
        2) 현재 보드에서 돌이 225개가 놓여있는 경우
##
    4) Action
        Action은 다음과 같이 구현합니다.
        1) Action은 0~224의 정수로 구성됩니다.
        2) Action은 15x15의 2차원 np.array로 구성됩니다.
        3) Action은 (x, y)의 형태로 구성됩니다.
##
    5) Info
        Info는 다음과 같이 구현합니다.
        1) Info는 현재 보드의 상태를 나타내는 15x15의 2차원 np.array로 구성됩니다.
        2) Info는 0, 1, -1로 구성되며, 0은 비어있음을 의미하고, 1은 흑돌, -1은 백돌을 의미합니다.
##
    6) Reset
        Reset은 다음과 같이 구현합니다.
        1) Reset은 현재 보드의 상태를 초기화합니다.
        2) Reset은 현재 보드의 상태를 반환합니다.
##
    보드 구현은 pygame 라이브러리를 사용합니다.
##













